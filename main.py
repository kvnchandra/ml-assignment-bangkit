# -*- coding: utf-8 -*-
"""temp.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G7kf4_5esbm_ok3NhFGBV20X5rRPheT7
"""

# Commented out IPython magic to ensure Python compatibility.
try:
  # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
except Exception:
  pass

# Import relevant modules
from __future__ import absolute_import, division, print_function, unicode_literals
import functools
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Download data using URL then save as "data.csv"
URL = "https://docs.google.com/spreadsheets/d/e/2PACX-1vQwPxotnLsgXuZt6tIXlAANttT3pxhP4ph0FhSjwvGUXfGKcnFRiqJORQg_9lBC77AH7QaxYypQ24nX/pub?output=csv"
dataset_path = keras.utils.get_file("data.csv", URL)
dataset_path

# Read data that had been dowloaded
dataframe = pd.read_csv(dataset_path)
dataframe.head()
dataframe.columns
dataframe.rename(columns={'concave points_mean': 'concave_points_mean',
                          'concave points_se' : 'concave_points_se',
                          'concave points_worst' : 'concave_points_worst'},
                 inplace=True)
dataframe.drop('id', axis=1, inplace=True)  # dropping the 'id' column
print("Row, Col", dataframe.shape)          # (row,col)
dataframe['diagnosis'] = dataframe['diagnosis'].map({'M':1, 'B':0})
dataframe.head()

# Separate data into 3 types
train, test = train_test_split(dataframe, 
                               test_size=0.2,
                               stratify=dataframe['diagnosis'])     # split data into same ratio based on diagnosis
print('Length of\n\tTrain Data\t: ', len(train),
      '\n\tTest Data\t: ', len(test))

# Inspect data
train.info()
train.describe()
# create percentage
colors_list = ['lightgreen', 'crimson']
explode_list = [0.1, 0]     # ratio for each class with which to offset each wedge.
train['diagnosis'].value_counts().plot(kind='pie',
                                       figsize=(6, 6),
                                       autopct='%1.1f%%', 
                                       startangle=90,    
                                       shadow=True,       
                                       labels=None,         # turn off labels on pie chart
                                       pctdistance=0.5,     # the ratio between the center of each pie slice and the start of the text generated by autopct 
                                       fontsize=15,
                                       colors=colors_list,  # add custom colors
                                       explode=explode_list 
                                       )
plt.title('Percentage of Malignant and Benign Case in Train Set', y = 1.05) 
plt.legend(labels=train['diagnosis'].value_counts().index, loc='upper left') 
plt.show()
# Create plot to display data
sns.pairplot(train[["diagnosis", "radius_mean",	"radius_se", "radius_worst"]],
             hue = 'diagnosis', 
             diag_kind="kde")
# Second, separate data into 2 types
train, val = train_test_split(train, 
                              test_size=0.2, 
                              stratify=train['diagnosis'])

print('Length of\n\tTrain Data\t: ', len(train),
      '\n\tValidation Data\t: ', len(val))

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('diagnosis')

    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))

    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
        
    return ds.batch(batch_size)

train_cols = list(train.columns)
train_cols.remove('diagnosis')
feature_columns = []

# numeric cols
for header in train_cols:
    feature_columns.append(tf.feature_column.numeric_column(header))

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

batch_size = 32

train_ds = df_to_dataset(train, batch_size=batch_size)
val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)

for feature_batch, label_batch in train_ds.take(1):
  print('Every feature:', list(feature_batch.keys()))
  print('A batch of targets:', label_batch )

model = tf.keras.Sequential([
    feature_layer,
    layers.Dense(128, activation='relu'),
    layers.Dense(128, activation='relu'),
    layers.Dense(1)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_ds,
          validation_data=val_ds,
          epochs=20)

def plot_graphs(history, metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric], '')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric, 'val_'+metric])
  plt.show()
  
plot_graphs(history, 'accuracy')
plot_graphs(history, 'loss')

loss, acc = model.evaluate(test_ds)
print('Loss\t\t:', loss, 
      '\nAccuracy\t:', acc)

predictions = model.predict(test_ds)
# Show some results
for prediction, survived in zip(predictions[:10], list(test_ds)[0][1][:10]):
  print("Predicted survival\t: {:.2%}\t".format(prediction[0]),
        " | Actual outcome\t: ",
        ("Malignant" if bool(survived) else "Benign"))